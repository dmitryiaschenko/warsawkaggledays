{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.options.display.float_format = '{:.8f}'.format\n",
    "import os, sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from fastai.imports import *\n",
    "from fastai.structured import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv', error_bad_lines=False, sep='\\t')\n",
    "train = pd.read_csv('train.csv', error_bad_lines=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>question_utc</th>\n",
       "      <th>question_text</th>\n",
       "      <th>question_score</th>\n",
       "      <th>answer_utc</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>330435</td>\n",
       "      <td>f48a5420fa6a66ecec95365dd67bdc99</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1512086400</td>\n",
       "      <td>All my buddies love Pacific Rim and always tal...</td>\n",
       "      <td>179</td>\n",
       "      <td>1512086616</td>\n",
       "      <td>I didn't really care for it, I thought the pac...</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1944975</td>\n",
       "      <td>f48a5420fa6a66ecec95365dd67bdc99</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1512086400</td>\n",
       "      <td>All my buddies love Pacific Rim and always tal...</td>\n",
       "      <td>179</td>\n",
       "      <td>1512090694</td>\n",
       "      <td>I thought it was hilarious, and I didn't reali...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2218735</td>\n",
       "      <td>f48a5420fa6a66ecec95365dd67bdc99</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1512086400</td>\n",
       "      <td>All my buddies love Pacific Rim and always tal...</td>\n",
       "      <td>179</td>\n",
       "      <td>1512090763</td>\n",
       "      <td>&amp;gt; Pacific Rim  The porno, or the studio rel...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2133251</td>\n",
       "      <td>f48a5420fa6a66ecec95365dd67bdc99</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1512086400</td>\n",
       "      <td>All my buddies love Pacific Rim and always tal...</td>\n",
       "      <td>179</td>\n",
       "      <td>1512091349</td>\n",
       "      <td>I enjoyed it more than most movies of that gen...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2133252</td>\n",
       "      <td>f48a5420fa6a66ecec95365dd67bdc99</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1512086400</td>\n",
       "      <td>All my buddies love Pacific Rim and always tal...</td>\n",
       "      <td>179</td>\n",
       "      <td>1512133012</td>\n",
       "      <td>Its terrible, its clearly made to be a self aw...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                       question_id  subreddit  question_utc  \\\n",
       "0   330435  f48a5420fa6a66ecec95365dd67bdc99  AskReddit    1512086400   \n",
       "1  1944975  f48a5420fa6a66ecec95365dd67bdc99  AskReddit    1512086400   \n",
       "2  2218735  f48a5420fa6a66ecec95365dd67bdc99  AskReddit    1512086400   \n",
       "3  2133251  f48a5420fa6a66ecec95365dd67bdc99  AskReddit    1512086400   \n",
       "4  2133252  f48a5420fa6a66ecec95365dd67bdc99  AskReddit    1512086400   \n",
       "\n",
       "                                       question_text  question_score  \\\n",
       "0  All my buddies love Pacific Rim and always tal...             179   \n",
       "1  All my buddies love Pacific Rim and always tal...             179   \n",
       "2  All my buddies love Pacific Rim and always tal...             179   \n",
       "3  All my buddies love Pacific Rim and always tal...             179   \n",
       "4  All my buddies love Pacific Rim and always tal...             179   \n",
       "\n",
       "   answer_utc                                        answer_text  answer_score  \n",
       "0  1512086616  I didn't really care for it, I thought the pac...            70  \n",
       "1  1512090694  I thought it was hilarious, and I didn't reali...            24  \n",
       "2  1512090763  &gt; Pacific Rim  The porno, or the studio rel...            11  \n",
       "3  1512091349  I enjoyed it more than most movies of that gen...            42  \n",
       "4  1512133012  Its terrible, its clearly made to be a self aw...             1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our target column and excluding it from our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((852885, 8), (663082, 8))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train['answer_score']\n",
    "train = train.drop(['answer_score'], axis=1)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning timeseries columns a datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['question_utc_transofrmed'] = pd.to_datetime(train['question_utc'],unit='s')\n",
    "train['answer_utc_transofrmed'] = pd.to_datetime(train['answer_utc'],unit='s')\n",
    "test['question_utc_transofrmed'] = pd.to_datetime(test['question_utc'],unit='s')\n",
    "test['answer_utc_transofrmed'] = pd.to_datetime(test['answer_utc'],unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new features from timseries like number of week, day, day of week, day of year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dateparts(df, fldname, drop=True):\n",
    "    fld = df[fldname]\n",
    "    if not np.issubdtype(fld.dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    for n in ('Week', 'Day', 'Dayofweek', 'Dayofyear'):\n",
    "        df[targ_pre+n] = getattr(fld.dt,n.lower())\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dateparts(train, 'answer_utc_transofrmed')\n",
    "add_dateparts(train, 'question_utc_transofrmed')\n",
    "add_dateparts(test, 'answer_utc_transofrmed')\n",
    "add_dateparts(test, 'question_utc_transofrmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['answ_quest_dif'] = train['answer_utc'] - train['question_utc']\n",
    "test['answ_quest_dif'] = test['answer_utc'] - test['question_utc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More feature engineering to extract more data from the text given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        super(FeatureEngineer, self).__init__()\n",
    "\n",
    "    def fit(self, data, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, *args, **kwargs):\n",
    "        data['log_len'] = np.log(data[\"answer_text\"].apply(len))\n",
    "        \n",
    "        data[\"question_mark_count\"] = [s.count(\"?\") for s in data[\"answer_text\"].values]\n",
    "        data[\"question_mark_present\"] = (data[\"question_mark_count\"] > 0).astype(int)\n",
    "        \n",
    "        data[\"exclamation_mark_count\"] = [s.count(\"!\") for s in data[\"answer_text\"].values]\n",
    "        data[\"exclamation_mark_present\"] = (data[\"exclamation_mark_count\"] > 0).astype(int)\n",
    "        \n",
    "        data[\"log_question_score\"] = np.log(data[\"question_score\"] + 1)\n",
    "        \n",
    "        def count_capitalized(text):\n",
    "            return sum(1 for c in text if c.isupper())\n",
    "\n",
    "        data[\"ratio_capitalized\"] = data[\"answer_text\"].apply(count_capitalized) / data[\"answer_text\"].apply(len)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureEngineer = FeatureEngineer()\n",
    "train = featureEngineer.transform(train)\n",
    "test = featureEngineer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_answers(df):\n",
    "    temp = (df['question_id'].value_counts()\n",
    "    .reset_index().rename(columns={'question_id':'answers_count', 'index':'question_id'}))\n",
    "    df = df.merge(temp, on=['question_id'],how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = n_answers(train)\n",
    "test = n_answers(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "img_regex = 'https?:[^)''\"]+\\.(?:jpg|jpeg|gif|png)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df[\"text_length\"] = df[\"answer_text\"].apply(lambda x: len(x))\n",
    "    df[\"answer_imgs\"] = df[\"answer_text\"].apply(lambda x: len(re.findall(img_regex, x))) #number of imgs in answer\n",
    "    df[\"answer_links\"] = df[\"answer_text\"].apply(lambda x: len(re.findall(url_regex, x))) #number of links  that are not imgs\n",
    "    df[\"answer_links\"] = df[\"answer_links\"] - df[\"answer_imgs\"]\n",
    "    df.answer_imgs = df.answer_imgs.apply(lambda x: 6 if x > 6 else x)\n",
    "    df.answer_links = df.answer_links.apply(lambda x: 10 if x > 10 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((852885, 28), (663082, 28))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vectors for NLP problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_text = pd.concat([train.answer_text, test.answer_text, train.question_text, test.question_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', \n",
    "                     analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                     ngram_range=(1, 1), stop_words='english', max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.fit(concat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_question_features = tv.transform(train.question_text)\n",
    "train_text_answer_features = tv.transform(train.answer_text)\n",
    "test_text_question_features = tv.transform(test.question_text)\n",
    "test_text_answer_features = tv.transform(test.answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer(sparse_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = pd.concat([train.question_id, test.question_id])\n",
    "subr = pd.concat([train.subreddit, test.subreddit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(qid)\n",
    "tr_qid = cv.transform(train.question_id)\n",
    "te_qid = cv.transform(test.question_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.fit(subr)\n",
    "tr_subr = lb.transform(train.subreddit)\n",
    "te_subr = lb.transform(test.subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((852885, 23), (663082, 23))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_d = train.drop(['question_text', 'answer_text', 'question_id', 'subreddit', 'id'],axis=1)\n",
    "test_d = test.drop(['question_text', 'answer_text', 'question_id', 'subreddit', 'id'], axis=1)\n",
    "train_d.shape, test_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking our dataset ang processed matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = hstack((train_text_question_features, train_text_answer_features, tr_subr, train_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = hstack((test_text_question_features, test_text_answer_features, te_subr, test_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\ttraining's rmse: 1.03785\tvalid_1's rmse: 1.03788\n",
      "[20]\ttraining's rmse: 0.999702\tvalid_1's rmse: 1.00004\n",
      "[30]\ttraining's rmse: 0.966954\tvalid_1's rmse: 0.967524\n",
      "[40]\ttraining's rmse: 0.938917\tvalid_1's rmse: 0.939722\n",
      "[50]\ttraining's rmse: 0.914935\tvalid_1's rmse: 0.915986\n",
      "[60]\ttraining's rmse: 0.894418\tvalid_1's rmse: 0.8957\n",
      "[70]\ttraining's rmse: 0.876901\tvalid_1's rmse: 0.878464\n",
      "[80]\ttraining's rmse: 0.861943\tvalid_1's rmse: 0.863772\n",
      "[90]\ttraining's rmse: 0.849143\tvalid_1's rmse: 0.851271\n",
      "[100]\ttraining's rmse: 0.838161\tvalid_1's rmse: 0.840586\n",
      "[110]\ttraining's rmse: 0.828723\tvalid_1's rmse: 0.831445\n",
      "[120]\ttraining's rmse: 0.820637\tvalid_1's rmse: 0.823657\n",
      "[130]\ttraining's rmse: 0.813656\tvalid_1's rmse: 0.816974\n",
      "[140]\ttraining's rmse: 0.807577\tvalid_1's rmse: 0.811219\n",
      "[150]\ttraining's rmse: 0.802319\tvalid_1's rmse: 0.80628\n",
      "[160]\ttraining's rmse: 0.797711\tvalid_1's rmse: 0.801947\n",
      "[170]\ttraining's rmse: 0.793663\tvalid_1's rmse: 0.798221\n",
      "[180]\ttraining's rmse: 0.790105\tvalid_1's rmse: 0.794955\n",
      "[190]\ttraining's rmse: 0.786894\tvalid_1's rmse: 0.792063\n",
      "[200]\ttraining's rmse: 0.784078\tvalid_1's rmse: 0.789553\n",
      "[210]\ttraining's rmse: 0.7815\tvalid_1's rmse: 0.78726\n",
      "[220]\ttraining's rmse: 0.779137\tvalid_1's rmse: 0.785199\n",
      "[230]\ttraining's rmse: 0.776973\tvalid_1's rmse: 0.783333\n",
      "[240]\ttraining's rmse: 0.774976\tvalid_1's rmse: 0.781671\n",
      "[250]\ttraining's rmse: 0.77312\tvalid_1's rmse: 0.780148\n",
      "[260]\ttraining's rmse: 0.771376\tvalid_1's rmse: 0.778746\n",
      "[270]\ttraining's rmse: 0.769776\tvalid_1's rmse: 0.77747\n",
      "[280]\ttraining's rmse: 0.768296\tvalid_1's rmse: 0.776262\n",
      "[290]\ttraining's rmse: 0.766863\tvalid_1's rmse: 0.775124\n",
      "[300]\ttraining's rmse: 0.765608\tvalid_1's rmse: 0.774168\n",
      "[310]\ttraining's rmse: 0.764348\tvalid_1's rmse: 0.77318\n",
      "[320]\ttraining's rmse: 0.763157\tvalid_1's rmse: 0.772276\n",
      "[330]\ttraining's rmse: 0.762066\tvalid_1's rmse: 0.771493\n",
      "[340]\ttraining's rmse: 0.760928\tvalid_1's rmse: 0.770639\n",
      "[350]\ttraining's rmse: 0.759857\tvalid_1's rmse: 0.769864\n",
      "[360]\ttraining's rmse: 0.758861\tvalid_1's rmse: 0.769152\n",
      "[370]\ttraining's rmse: 0.757938\tvalid_1's rmse: 0.768504\n",
      "[380]\ttraining's rmse: 0.757075\tvalid_1's rmse: 0.767865\n",
      "[390]\ttraining's rmse: 0.75629\tvalid_1's rmse: 0.767318\n",
      "[400]\ttraining's rmse: 0.755534\tvalid_1's rmse: 0.766827\n",
      "[410]\ttraining's rmse: 0.754835\tvalid_1's rmse: 0.766368\n",
      "[420]\ttraining's rmse: 0.753997\tvalid_1's rmse: 0.765766\n",
      "[430]\ttraining's rmse: 0.753298\tvalid_1's rmse: 0.765308\n",
      "[440]\ttraining's rmse: 0.752554\tvalid_1's rmse: 0.764815\n",
      "[450]\ttraining's rmse: 0.751834\tvalid_1's rmse: 0.764309\n",
      "[460]\ttraining's rmse: 0.751162\tvalid_1's rmse: 0.763849\n",
      "[470]\ttraining's rmse: 0.750574\tvalid_1's rmse: 0.76349\n",
      "[480]\ttraining's rmse: 0.749949\tvalid_1's rmse: 0.763063\n",
      "[490]\ttraining's rmse: 0.749291\tvalid_1's rmse: 0.762618\n",
      "[500]\ttraining's rmse: 0.748698\tvalid_1's rmse: 0.762244\n",
      "[510]\ttraining's rmse: 0.748167\tvalid_1's rmse: 0.761937\n",
      "[520]\ttraining's rmse: 0.747626\tvalid_1's rmse: 0.761598\n",
      "[530]\ttraining's rmse: 0.747123\tvalid_1's rmse: 0.761308\n",
      "[540]\ttraining's rmse: 0.746624\tvalid_1's rmse: 0.761011\n",
      "[550]\ttraining's rmse: 0.746092\tvalid_1's rmse: 0.760674\n",
      "[560]\ttraining's rmse: 0.745625\tvalid_1's rmse: 0.760399\n",
      "[570]\ttraining's rmse: 0.745176\tvalid_1's rmse: 0.760124\n",
      "[580]\ttraining's rmse: 0.744759\tvalid_1's rmse: 0.759901\n",
      "[590]\ttraining's rmse: 0.744337\tvalid_1's rmse: 0.759674\n",
      "[600]\ttraining's rmse: 0.743901\tvalid_1's rmse: 0.759431\n",
      "[610]\ttraining's rmse: 0.743484\tvalid_1's rmse: 0.759195\n",
      "[620]\ttraining's rmse: 0.743098\tvalid_1's rmse: 0.758978\n",
      "[630]\ttraining's rmse: 0.742695\tvalid_1's rmse: 0.758759\n",
      "[640]\ttraining's rmse: 0.74235\tvalid_1's rmse: 0.758585\n",
      "[650]\ttraining's rmse: 0.741997\tvalid_1's rmse: 0.758397\n",
      "[660]\ttraining's rmse: 0.741608\tvalid_1's rmse: 0.758189\n",
      "[670]\ttraining's rmse: 0.741289\tvalid_1's rmse: 0.758018\n",
      "[680]\ttraining's rmse: 0.740928\tvalid_1's rmse: 0.75782\n",
      "[690]\ttraining's rmse: 0.740633\tvalid_1's rmse: 0.75768\n",
      "[700]\ttraining's rmse: 0.740241\tvalid_1's rmse: 0.757468\n",
      "[710]\ttraining's rmse: 0.739965\tvalid_1's rmse: 0.757332\n",
      "[720]\ttraining's rmse: 0.739677\tvalid_1's rmse: 0.757189\n",
      "[730]\ttraining's rmse: 0.739311\tvalid_1's rmse: 0.756984\n",
      "[740]\ttraining's rmse: 0.739047\tvalid_1's rmse: 0.756861\n",
      "[750]\ttraining's rmse: 0.738673\tvalid_1's rmse: 0.756645\n",
      "[760]\ttraining's rmse: 0.738399\tvalid_1's rmse: 0.756509\n",
      "[770]\ttraining's rmse: 0.73815\tvalid_1's rmse: 0.756387\n",
      "[780]\ttraining's rmse: 0.73786\tvalid_1's rmse: 0.75624\n",
      "[790]\ttraining's rmse: 0.737612\tvalid_1's rmse: 0.756119\n",
      "[800]\ttraining's rmse: 0.737328\tvalid_1's rmse: 0.755982\n",
      "[810]\ttraining's rmse: 0.737076\tvalid_1's rmse: 0.755864\n",
      "[820]\ttraining's rmse: 0.736858\tvalid_1's rmse: 0.755762\n",
      "[830]\ttraining's rmse: 0.736635\tvalid_1's rmse: 0.755662\n",
      "[840]\ttraining's rmse: 0.736411\tvalid_1's rmse: 0.755577\n",
      "[850]\ttraining's rmse: 0.736172\tvalid_1's rmse: 0.755463\n",
      "[860]\ttraining's rmse: 0.735943\tvalid_1's rmse: 0.755355\n",
      "[870]\ttraining's rmse: 0.735661\tvalid_1's rmse: 0.755204\n",
      "[880]\ttraining's rmse: 0.735464\tvalid_1's rmse: 0.755123\n",
      "[890]\ttraining's rmse: 0.735232\tvalid_1's rmse: 0.755003\n",
      "[900]\ttraining's rmse: 0.735057\tvalid_1's rmse: 0.754927\n",
      "[910]\ttraining's rmse: 0.734878\tvalid_1's rmse: 0.75485\n",
      "[920]\ttraining's rmse: 0.73469\tvalid_1's rmse: 0.754775\n",
      "[930]\ttraining's rmse: 0.734466\tvalid_1's rmse: 0.754667\n",
      "[940]\ttraining's rmse: 0.734265\tvalid_1's rmse: 0.75457\n",
      "[950]\ttraining's rmse: 0.734085\tvalid_1's rmse: 0.754505\n",
      "[960]\ttraining's rmse: 0.733929\tvalid_1's rmse: 0.754434\n",
      "[970]\ttraining's rmse: 0.733713\tvalid_1's rmse: 0.754326\n",
      "[980]\ttraining's rmse: 0.733567\tvalid_1's rmse: 0.754275\n",
      "[990]\ttraining's rmse: 0.733395\tvalid_1's rmse: 0.754198\n",
      "[1000]\ttraining's rmse: 0.733227\tvalid_1's rmse: 0.754141\n",
      "[1010]\ttraining's rmse: 0.733064\tvalid_1's rmse: 0.754078\n",
      "[1020]\ttraining's rmse: 0.732906\tvalid_1's rmse: 0.754003\n",
      "[1030]\ttraining's rmse: 0.732715\tvalid_1's rmse: 0.753921\n",
      "[1040]\ttraining's rmse: 0.732539\tvalid_1's rmse: 0.753851\n",
      "[1050]\ttraining's rmse: 0.732336\tvalid_1's rmse: 0.753761\n",
      "[1060]\ttraining's rmse: 0.732178\tvalid_1's rmse: 0.753696\n",
      "[1070]\ttraining's rmse: 0.732019\tvalid_1's rmse: 0.753639\n",
      "[1080]\ttraining's rmse: 0.731827\tvalid_1's rmse: 0.753554\n",
      "[1090]\ttraining's rmse: 0.731659\tvalid_1's rmse: 0.753481\n",
      "[1100]\ttraining's rmse: 0.731436\tvalid_1's rmse: 0.753371\n",
      "[1110]\ttraining's rmse: 0.731279\tvalid_1's rmse: 0.753308\n",
      "[1120]\ttraining's rmse: 0.731148\tvalid_1's rmse: 0.753259\n",
      "[1130]\ttraining's rmse: 0.730972\tvalid_1's rmse: 0.753182\n",
      "[1140]\ttraining's rmse: 0.730841\tvalid_1's rmse: 0.753132\n",
      "[1150]\ttraining's rmse: 0.730663\tvalid_1's rmse: 0.753057\n",
      "[1160]\ttraining's rmse: 0.730548\tvalid_1's rmse: 0.75301\n",
      "[1170]\ttraining's rmse: 0.730393\tvalid_1's rmse: 0.752947\n",
      "[1180]\ttraining's rmse: 0.73026\tvalid_1's rmse: 0.752894\n",
      "[1190]\ttraining's rmse: 0.730105\tvalid_1's rmse: 0.752827\n",
      "[1200]\ttraining's rmse: 0.729919\tvalid_1's rmse: 0.752742\n",
      "[1210]\ttraining's rmse: 0.729811\tvalid_1's rmse: 0.752702\n",
      "[1220]\ttraining's rmse: 0.729659\tvalid_1's rmse: 0.752654\n",
      "[1230]\ttraining's rmse: 0.729492\tvalid_1's rmse: 0.752576\n",
      "[1240]\ttraining's rmse: 0.729383\tvalid_1's rmse: 0.752531\n",
      "[1250]\ttraining's rmse: 0.729241\tvalid_1's rmse: 0.752468\n",
      "[1260]\ttraining's rmse: 0.729123\tvalid_1's rmse: 0.752429\n",
      "[1270]\ttraining's rmse: 0.728968\tvalid_1's rmse: 0.752366\n",
      "[1280]\ttraining's rmse: 0.728843\tvalid_1's rmse: 0.752315\n",
      "[1290]\ttraining's rmse: 0.728726\tvalid_1's rmse: 0.752276\n",
      "[1300]\ttraining's rmse: 0.728588\tvalid_1's rmse: 0.752213\n",
      "[1310]\ttraining's rmse: 0.728458\tvalid_1's rmse: 0.752165\n",
      "[1320]\ttraining's rmse: 0.728352\tvalid_1's rmse: 0.75213\n",
      "[1330]\ttraining's rmse: 0.728192\tvalid_1's rmse: 0.752059\n",
      "[1340]\ttraining's rmse: 0.728028\tvalid_1's rmse: 0.751988\n",
      "[1350]\ttraining's rmse: 0.727879\tvalid_1's rmse: 0.751926\n",
      "[1360]\ttraining's rmse: 0.727742\tvalid_1's rmse: 0.75186\n",
      "[1370]\ttraining's rmse: 0.727592\tvalid_1's rmse: 0.751801\n",
      "[1380]\ttraining's rmse: 0.727482\tvalid_1's rmse: 0.751768\n",
      "[1390]\ttraining's rmse: 0.727359\tvalid_1's rmse: 0.751716\n",
      "[1400]\ttraining's rmse: 0.727219\tvalid_1's rmse: 0.751664\n",
      "[1410]\ttraining's rmse: 0.727101\tvalid_1's rmse: 0.751622\n",
      "[1420]\ttraining's rmse: 0.72701\tvalid_1's rmse: 0.751599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1430]\ttraining's rmse: 0.726877\tvalid_1's rmse: 0.751551\n",
      "[1440]\ttraining's rmse: 0.726748\tvalid_1's rmse: 0.751499\n",
      "[1450]\ttraining's rmse: 0.726637\tvalid_1's rmse: 0.751459\n",
      "[1460]\ttraining's rmse: 0.726538\tvalid_1's rmse: 0.751424\n",
      "[1470]\ttraining's rmse: 0.726412\tvalid_1's rmse: 0.751375\n",
      "[1480]\ttraining's rmse: 0.726271\tvalid_1's rmse: 0.751314\n",
      "[1490]\ttraining's rmse: 0.726161\tvalid_1's rmse: 0.751282\n",
      "[1500]\ttraining's rmse: 0.726028\tvalid_1's rmse: 0.751225\n",
      "[1510]\ttraining's rmse: 0.725904\tvalid_1's rmse: 0.751186\n",
      "[1520]\ttraining's rmse: 0.725788\tvalid_1's rmse: 0.751139\n",
      "[1530]\ttraining's rmse: 0.72564\tvalid_1's rmse: 0.751067\n",
      "[1540]\ttraining's rmse: 0.725525\tvalid_1's rmse: 0.751019\n",
      "[1550]\ttraining's rmse: 0.725406\tvalid_1's rmse: 0.750985\n",
      "[1560]\ttraining's rmse: 0.725295\tvalid_1's rmse: 0.750951\n",
      "[1570]\ttraining's rmse: 0.725182\tvalid_1's rmse: 0.750906\n",
      "[1580]\ttraining's rmse: 0.725085\tvalid_1's rmse: 0.750869\n",
      "[1590]\ttraining's rmse: 0.724982\tvalid_1's rmse: 0.75083\n",
      "[1600]\ttraining's rmse: 0.724875\tvalid_1's rmse: 0.750782\n",
      "[1610]\ttraining's rmse: 0.724763\tvalid_1's rmse: 0.750744\n",
      "[1620]\ttraining's rmse: 0.724646\tvalid_1's rmse: 0.750691\n",
      "[1630]\ttraining's rmse: 0.724529\tvalid_1's rmse: 0.750652\n",
      "[1640]\ttraining's rmse: 0.724387\tvalid_1's rmse: 0.750589\n",
      "[1650]\ttraining's rmse: 0.724279\tvalid_1's rmse: 0.750549\n",
      "[1660]\ttraining's rmse: 0.724152\tvalid_1's rmse: 0.750506\n",
      "[1670]\ttraining's rmse: 0.724043\tvalid_1's rmse: 0.75047\n",
      "[1680]\ttraining's rmse: 0.723936\tvalid_1's rmse: 0.75043\n",
      "[1690]\ttraining's rmse: 0.723821\tvalid_1's rmse: 0.750391\n",
      "[1700]\ttraining's rmse: 0.723711\tvalid_1's rmse: 0.750347\n",
      "[1710]\ttraining's rmse: 0.723605\tvalid_1's rmse: 0.75031\n",
      "[1720]\ttraining's rmse: 0.723495\tvalid_1's rmse: 0.750273\n",
      "[1730]\ttraining's rmse: 0.723411\tvalid_1's rmse: 0.750247\n",
      "[1740]\ttraining's rmse: 0.723305\tvalid_1's rmse: 0.750209\n",
      "[1750]\ttraining's rmse: 0.723174\tvalid_1's rmse: 0.750151\n",
      "[1760]\ttraining's rmse: 0.723074\tvalid_1's rmse: 0.750122\n",
      "[1770]\ttraining's rmse: 0.722928\tvalid_1's rmse: 0.750065\n",
      "[1780]\ttraining's rmse: 0.722839\tvalid_1's rmse: 0.750031\n",
      "[1790]\ttraining's rmse: 0.722751\tvalid_1's rmse: 0.750007\n",
      "[1800]\ttraining's rmse: 0.72267\tvalid_1's rmse: 0.749977\n",
      "[1810]\ttraining's rmse: 0.722557\tvalid_1's rmse: 0.749933\n",
      "[1820]\ttraining's rmse: 0.722438\tvalid_1's rmse: 0.749883\n",
      "[1830]\ttraining's rmse: 0.722353\tvalid_1's rmse: 0.749859\n",
      "[1840]\ttraining's rmse: 0.72226\tvalid_1's rmse: 0.749827\n",
      "[1850]\ttraining's rmse: 0.722149\tvalid_1's rmse: 0.749787\n",
      "[1860]\ttraining's rmse: 0.722067\tvalid_1's rmse: 0.749763\n",
      "[1870]\ttraining's rmse: 0.721971\tvalid_1's rmse: 0.749731\n",
      "[1880]\ttraining's rmse: 0.721871\tvalid_1's rmse: 0.749688\n",
      "[1890]\ttraining's rmse: 0.721758\tvalid_1's rmse: 0.749633\n",
      "[1900]\ttraining's rmse: 0.721661\tvalid_1's rmse: 0.749602\n",
      "[1910]\ttraining's rmse: 0.721566\tvalid_1's rmse: 0.749566\n",
      "[1920]\ttraining's rmse: 0.721477\tvalid_1's rmse: 0.749536\n",
      "[1930]\ttraining's rmse: 0.721352\tvalid_1's rmse: 0.749499\n",
      "[1940]\ttraining's rmse: 0.721268\tvalid_1's rmse: 0.749468\n",
      "[1950]\ttraining's rmse: 0.721173\tvalid_1's rmse: 0.749444\n",
      "[1960]\ttraining's rmse: 0.721085\tvalid_1's rmse: 0.749422\n",
      "[1970]\ttraining's rmse: 0.72099\tvalid_1's rmse: 0.749394\n",
      "[1980]\ttraining's rmse: 0.720895\tvalid_1's rmse: 0.74936\n",
      "[1990]\ttraining's rmse: 0.720786\tvalid_1's rmse: 0.749316\n",
      "[2000]\ttraining's rmse: 0.720676\tvalid_1's rmse: 0.749279\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining's rmse: 0.720676\tvalid_1's rmse: 0.749279\n"
     ]
    }
   ],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(x, np.log1p(y), test_size = 0.15, random_state = 42) \n",
    "d_train = lgb.Dataset(train_X, label=train_y)\n",
    "d_valid = lgb.Dataset(valid_X, label=valid_y)\n",
    "watchlist = [d_train, d_valid]\n",
    "params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'application': 'regression',\n",
    "        'max_depth': 12,\n",
    "        'num_leaves': 140,\n",
    "        'verbosity': -1,\n",
    "        'metric': 'RMSE',\n",
    "        'data_random_seed': 1,\n",
    "        'bagging_fraction': 1,\n",
    "        'nthread': 8\n",
    "        \n",
    "}\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=2000, valid_sets=watchlist, \\\n",
    "    early_stopping_rounds=50, verbose_eval=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predL = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.read_csv('sample_submission.csv')\n",
    "s.answer_score = np.expm1(predL)\n",
    "s.to_csv('4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
